# 2526-m2qdcs-tencomp

Please make sure that your name appears in the list of registered students (if not please add it): https://cirrus.universite-paris-saclay.fr/s/2n5beYA2eMMoXmJ

## Session 1: Overview of matrix computations and tensors

## Session 2: Matrix computations (cont.) and tensor decompositions

## Session 3: Tensor decompositions (cont.)

## Session 4: Tensor networks and algorithms

## Session 5: Tensor network computational challenges, applications, algorithms (+mini exam)

## Session 6: Lab session

## Session 7: Research paper presentations

# Evaluation

Your course grade will be calculated as follows:

1. Graded lab assignment (5/20)

2. In-class mini-exam (5/20)

3. Research paper presentation (10/20)

* In-class mini-exam will be on the subjects covered in the first five sessions. You can bring ** four A4 sheets ** (eight pages recto-verso) with your notes to use during the exam. No other material or electronics is allowed. The exam will be around 1 hour, at the end of Session 6.

* Research paper list will be announced later in the course.

* Session 7 will be held in two seperate slots (one for QDCS one for QMI students. You are welcome to join both if your calendar permits).

# Papers for presentation

## General rules and guidelines
* For presentations, please find a partner/binome. If you cannot find a partner, you can present solo but this should be a last resort only.
* For QDCS students, send me **one e-mail per group** indicating the names of two people in your group, and ordered list of of articles in the order of your preference. You must include **all articles in A? and B? groups** (see below).
* For QMI students, send me **one e-mail per group** indicating the names of two people in your group, and ordered list of of articles in the order of your preference. You must include **all articles in C? and B? groups** (see below).
* I need the list by **17/02 class session** so we can fix the assignments.
* You will have 15 minutes to present + 5-10 minutes of questions. Do not spent time on unnecessary tensor and decompositions background that we saw in class; dive straight into the application/method/algorithm/implementation etc.


## HPC/Computing

A1 Application performance modeling via tensor completion

A2 Accelerating Tensor-train Decomposition on Graph Neural Networks

A3 Accelerating sparse tensor decomposition using adaptive linearized representation

A4 Parallel tensor compression for large-scale scientific data

A5 Parallel nonnegative CP decomposition of dense tensors

A6 Distributed-Memory DMRG via Sparse and Dense Parallel Tensor Contractions

A7 Scalable Sparse Tensor Decompositions in Distributed Memory Systems

A8 An exploration of optimization algorithms for high performance tensor completion



## Tensor numerical methods/algorithms/simulations/contractions

B1 Low-rank explicit qtt representation of the laplace operator and its inverse

B2 TT-cross approximation for multidimensional arrays

B3 Mixed precision iterative refinement for low-rank matrix and tensor approximations

B4 Near-Optimal Contraction Strategies for the Scalar Product in the Tensor-Train Format

B5 Discovering faster matrix multiplication algorithms with reinforcement learning

B6 Speeding-up convolutional neural networks using fine-tuned cp-decomposition (1-person)

B7 A massively parallel tensor contraction framework for coupled-cluster computations



## Quantum computing / applications

C1 Simulating quantum computation by contracting tensor networks

C2 Hyper-optimized tensor network contraction

C3 Functional Tensor-Train Chebyshev Method for Multidimensional Quantum Dynamics Simulations

C4 Tensor-train split-operator Fourier transform (TT-SOFT) method: Multidimensional nonadiabatic quantum dynamics

C5 Synergistic pretraining of parametrized quantum circuits via tensor networks

C6 Towards quantum machine learning with tensor networks

C7 Quantum circuit matrix product state ansatz for large-scale simulations of molecules

C8 Ultrafast ab-initio Quantum Chemistry Using Matrix Product States


# References

* Tensor Decompositions for Data Science: https://users.wfu.edu/ballard/pdfs/tensor_textbook.pdf

* Tensor Ranks for the Pedestrian for Dimension Reduction and Disentangling Interactions. Alain Franc, 2021: https://arxiv.org/pdf/2201.07473
